<h1 align="center">Text-Generation-Comparison</h1>

# ğŸ¤– Text Generation Decoding Strategies with GPT-2
This project explores and compares different **text generation decoding strategies** using the GPT-2 language model.  
The goal is to better understand how methods like sampling and beam search influence the **diversity**, **fluency**, and **coherence** of generated text.

---

## ğŸ§  Decoding Strategies Explained
### ğŸ”¹ Top-k Sampling
Randomly selects the next word from the **top k most likely tokens**.  
âœ… Generates diverse results  
âš ï¸ May lose coherence if k is too high

### ğŸ”¹ Top-p (Nucleus) Sampling
Samples from the **smallest set of tokens** whose cumulative probability â‰¥ *p*.  
âœ… Balances randomness and relevance  
âš ï¸ Requires careful tuning of *p*

### ğŸ”¸ Temperature (ğŸ”¥ Sampling Sharpness)
**Controls the confidence of the model when sampling.**

- Low values (e.g., `0.7`) â†’ safer, more predictable outputs
- High values (e.g., `1.2`) â†’ more diverse, creative, but potentially incoherent
- **Used in combination** with top-k and top-p

> Example: Top-p with temperature = 0.8 produces more grounded text than with temperature = 1.2

### ğŸ”¹ Beam Search
Generates multiple candidate sequences and selects the **most probable one overall**.  
âœ… Often coherent and grammatical  
âš ï¸ May produce repetitive or dull text

### ğŸ”¹ Beam Search with N-gram Blocking
Like beam search, but blocks repeated **n-grams** (e.g. â€œthe cat the catâ€).  
âœ… Reduces repetition  
âš ï¸ Slightly slower and more complex

ğŸ“š Resources:  
- [Hugging Face blog on decoding](https://huggingface.co/blog/how-to-generate)  
- [Top-k and Top-p Sampling Paper](https://arxiv.org/abs/1904.09751)

---

## ğŸ“‹ Summary Comparison of Decoding Methods

| Method               | Diversity ğŸ”€ | Fluency ğŸ§  | Repetition ğŸ” | Speed âš¡ | Notes                                  |
|----------------------|--------------|------------|----------------|----------|----------------------------------------|
| Top-k Sampling       | High         | Medium     | Low            | Fast     | Needs `k` & `temperature` tuning           |
| Top-p Sampling       | High         | Medium     | Lowâ€“Medium     | Fast     | Adaptive; tune with `p` + `temperature`      |
| Beam Search          | Low          | High       | High           | Slow     | Safe but often repetitive              |
| Beam + N-gram Block  | Medium       | High       | Low            | Slow     | Improves beam search diversity         |

## ğŸ’¡ When to Use What
âœ… Use Top-p Sampling for creative tasks like story or poem generation

âœ… Use Beam Search for structured output like summarization or question answering

âœ… Use Top-k if you want fast generation with moderate randomness

âœ… Use Beam + N-gram Blocking to reduce repetition in long-form generation


